{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a26daf2",
   "metadata": {},
   "source": [
    "#### Data Analysis 2\n",
    "\n",
    "### Assignment 2\n",
    "Alejandra Savage Briz\n",
    "\n",
    "\n",
    "Nicolás Fernandez\n",
    "\n",
    "\n",
    " \n",
    "**Central European University**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a965aa",
   "metadata": {},
   "source": [
    "#### Importing py_helper_funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31243a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "####################################################\n",
    "import copy\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "\n",
    "####################################################\n",
    "# Define global vars\n",
    "####################################################\n",
    "color = [\"#3a5e8cFF\", \"#10a53dFF\", \"#541352FF\", \"#ffcf20FF\", \"#2f9aa0FF\"]\n",
    "\n",
    "\n",
    "####################################################\n",
    "# Define helper functions\n",
    "####################################################\n",
    "def seq(start: float, stop: float, by: float, round_n=3) -> list:\n",
    "    \"\"\"\n",
    "    Custom function, used for setting the breaks of plotnine scales.\n",
    "\n",
    "       Parameters\n",
    "    ----------\n",
    "    start : float\n",
    "        Start of the breaks.\n",
    "    stop : float\n",
    "        End of the breaks.\n",
    "    by : float\n",
    "        Steps between breaks.\n",
    "    round_n: int, default=3\n",
    "        Decimals to round floats in output.\n",
    "    \"\"\"\n",
    "    epsilon = np.finfo(\"float\").eps\n",
    "\n",
    "    return [\n",
    "        round(x, round_n) for x in list(np.arange(start, stop + (by - epsilon), by))\n",
    "    ]\n",
    "\n",
    "\n",
    "def skew(l: npt.ArrayLike, round_n=3) -> float:\n",
    "    return round((np.mean(l) - np.median(l)) / np.std(l), round_n)\n",
    "\n",
    "\n",
    "def knot_ceil(vector: np.array, knot: float) -> np.array:\n",
    "    vector_copy = copy.deepcopy(vector)\n",
    "    vector_copy[vector_copy > knot] = knot\n",
    "    return vector_copy\n",
    "\n",
    "\n",
    "def lspline(series: pd.Series, knots: List[float]) -> np.array:\n",
    "    \"\"\"\n",
    "    Function to create design matrix to esitmate a piecewise\n",
    "    linear spline regression.\n",
    "\n",
    "       Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Your variable in a pandas Series.\n",
    "    knots : List[float]\n",
    "        The knots, that result in n + 1 line segments.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(knots) != list:\n",
    "        knots = [knots]\n",
    "    design_matrix = None\n",
    "    vector = series.values\n",
    "\n",
    "    for i in range(len(knots)):\n",
    "        # print(i)\n",
    "        # print(vector)\n",
    "        if i == 0:\n",
    "            column = knot_ceil(vector, knots[i])\n",
    "        else:\n",
    "            column = knot_ceil(vector, knots[i] - knots[i - 1])\n",
    "        # print(column)\n",
    "        if i == 0:\n",
    "            design_matrix = column\n",
    "        else:\n",
    "            design_matrix = np.column_stack((design_matrix, column))\n",
    "        # print(design_matrix)\n",
    "        vector = vector - column\n",
    "    design_matrix = np.column_stack((design_matrix, vector))\n",
    "    # print(design_matrix)\n",
    "    return design_matrix\n",
    "\n",
    "\n",
    "def create_calibration_plot(\n",
    "    data: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    prob_var: str,\n",
    "    actual_var: str,\n",
    "    y_lab=\"Actual event probability\",\n",
    "    n_bins=10,\n",
    "    breaks=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to create calibration curve.\n",
    "    Returns calibration curve on a plot.\n",
    "\n",
    "       Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Your dataframe, containing the actual outcome and\n",
    "        the predicted probabilities of that outcome\n",
    "        by a model.\n",
    "    file_name : str\n",
    "        Filename to save. NOTE: this is note used for now.\n",
    "    prob_var : str\n",
    "        Name of the variable, containin predicted\n",
    "        probabilities.\n",
    "    actual_var : str\n",
    "        Name of the actual outcome variable.\n",
    "    y_lab: str\n",
    "        Label on y axis of the plot.\n",
    "    n_bins : int, default=10\n",
    "        The number of bins, you would like to create.\n",
    "        This is because with many values for the predicted probabilities,\n",
    "        we may have few observations to look at for each value.\n",
    "    breaks (optional): list or None\n",
    "        You can speficy the breaks of the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    if breaks is None:\n",
    "        breaks = np.around(\n",
    "            np.linspace(0, (n_bins + 1) / 10, num=n_bins + 1, endpoint=False),\n",
    "            decimals=1,\n",
    "        ).tolist()\n",
    "\n",
    "    data[\"prob_bin\"] = pd.cut(data[prob_var], breaks, right=True, include_lowest=True)\n",
    "\n",
    "    binned_data = (\n",
    "        data.groupby(\"prob_bin\")\n",
    "        .agg(\n",
    "            mean_prob=(prob_var, \"mean\"),\n",
    "            mean_actual=(actual_var, \"mean\"),\n",
    "            n=(actual_var, \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        ggplot(binned_data, aes(\"mean_prob\", \"mean_actual\"))\n",
    "        + geom_line(color=color[0], size=1, show_legend=True)\n",
    "        + geom_point(color=color[0], size=1, alpha=0.7, show_legend=False, na_rm=True)\n",
    "        + geom_segment(\n",
    "            x=min(breaks),\n",
    "            xend=max(breaks),\n",
    "            y=min(breaks),\n",
    "            yend=max(breaks),\n",
    "            color=color[1],\n",
    "            size=0.5,\n",
    "        )\n",
    "        + theme_bw()\n",
    "        + labs(x=\"Predicted event probability\", y=y_lab)\n",
    "        + coord_cartesian(xlim=(0, 1), ylim=(0, 1))\n",
    "        + expand_limits(x=0.01, y=0.01)\n",
    "        + scale_y_continuous(expand=(0.01, 0.01), breaks=(seq(0, 1.1, 0.1)))\n",
    "        + scale_x_continuous(expand=(0.01, 0.01), breaks=(seq(0, 1.1, 0.1)))\n",
    "    )\n",
    "\n",
    "\n",
    "def poly(x: npt.ArrayLike, degree=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fit polynomial.\n",
    "\n",
    "    These are non orthogonal factors, but it may not matter if\n",
    "    we only need this for predictions (without interpreting the\n",
    "    coefficients) or visualisation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : npt.ArrayLike\n",
    "        Data array.\n",
    "    degree : int, default=1\n",
    "        Degree of the polynomial.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for i in range(degree + 1):\n",
    "        if i == 1:\n",
    "            d[\"x\"] = x\n",
    "        else:\n",
    "            d[f\"x**{i}\"] = np.power(x, i)\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "\n",
    "def price_diff_by_variables(\n",
    "    df: pd.DataFrame, factor_var: str, dummy_var: str, factor_lab: str, dummy_lab: str\n",
    ") -> ggplot:\n",
    "    \"\"\"\n",
    "    Price difference by selected factor and dummy variables.\n",
    "\n",
    "    This function creates a barplots looking for interactions.\n",
    "    Used in `ch14-airbnb-prediction.ipynb`.\n",
    "\n",
    "        Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Your dataframe.\n",
    "    factor_var : str\n",
    "        Your factor variable (like room_type).\n",
    "    dummy_var : str\n",
    "        The dummy variable you are interested in (like TV).\n",
    "    factor_lab : str\n",
    "        The label on the final plot for the `factor_var`.\n",
    "    dummy_lab : str\n",
    "        The label on the final plot for the `dummy_var`.\n",
    "    \"\"\"\n",
    "\n",
    "    stats = df.groupby([factor_var, dummy_var]).agg(\n",
    "        Mean=(\"price\", np.mean), sd=(\"price\", np.std), size=(\"price\", \"size\")\n",
    "    )\n",
    "    stats[\"se\"] = stats[\"sd\"] / stats[\"size\"] ** (1 / 2)\n",
    "    stats[\"Mean_l\"] = stats[\"Mean\"] - (1.96 * stats[\"se\"])\n",
    "    stats[\"Mean_u\"] = stats[\"Mean\"] + (1.96 * stats[\"se\"])\n",
    "    stats = stats.drop([\"sd\", \"size\"], axis=1).reset_index()\n",
    "\n",
    "    return (\n",
    "        ggplot(\n",
    "            stats,\n",
    "            aes(\n",
    "                stats.columns[0],\n",
    "                stats.columns[2],\n",
    "                fill=\"factor(\" + stats.columns[1] + \")\",\n",
    "            ),\n",
    "        )\n",
    "        + geom_bar(stat=\"identity\", position=position_dodge(width=0.9))\n",
    "        + geom_errorbar(\n",
    "            aes(ymin=\"Mean_l\", ymax=\"Mean_u\"),\n",
    "            position=position_dodge(width=0.9),\n",
    "            width=0.25,\n",
    "        )\n",
    "        + scale_color_manual(name=dummy_lab, values=(color[1], color[0]))\n",
    "        + scale_fill_manual(name=dummy_lab, values=(color[1], color[0]))\n",
    "        + ylab(\"Mean Price\")\n",
    "        + xlab(factor_lab)\n",
    "        + theme_bw()\n",
    "        + theme(\n",
    "            panel_grid_major=element_blank(),\n",
    "            panel_grid_minor=element_blank(),\n",
    "            panel_border=element_blank(),\n",
    "            axis_line=element_line(),\n",
    "            legend_position=\"top\",\n",
    "            legend_box=\"vertical\",\n",
    "            legend_text=element_text(size=5),\n",
    "            legend_title=element_text(size=5, face=\"bold\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "\n",
    "def ols_crossvalidator(\n",
    "    formula: str, data: pd.DataFrame, n_folds=5, average_rmse=True\n",
    ") -> dict:\n",
    "    \"\"\"OLS cross-validator\n",
    "\n",
    "\n",
    "    Estimates `formula` equation with OLS and returns values of RMSE, R`2, No. coefficients,\n",
    "    BIC on `data`. Does k-fold cross-validation and either returns train and test RMSE for each\n",
    "    fold, or return averarage train and test RMSEs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    formula : str\n",
    "        Equation that is estimated by OLS.\n",
    "    data : pd.DataFrame\n",
    "        Database in a wide format.\n",
    "    n_folds : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    average_rmse : bool, default=True\n",
    "        Whether to return the average train and test RMSE of the k-fold CV, or return\n",
    "        train and test RMSE-s for each fold.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dependent variable\n",
    "\n",
    "    y = formula.split(\"~\")[0].strip()\n",
    "\n",
    "    # Get statistics on the whole work data\n",
    "\n",
    "    model = smf.ols(formula, data=data).fit()\n",
    "\n",
    "    rsquared = model.rsquared\n",
    "    # n_coefficients = model.params.shape[0]\n",
    "    n_coefficients = (\n",
    "        model.df_model + 1\n",
    "    )  # This might differ from model.params.shape[0], because of collinear predictors\n",
    "    bic = model.bic\n",
    "    rmse_alldata = rmse(model.predict(), data[y])\n",
    "\n",
    "    # Calculating test and train RMSE-s for each fold\n",
    "\n",
    "    k = KFold(n_splits=n_folds, shuffle=False, random_state=None)\n",
    "\n",
    "    rmse_train = []\n",
    "    rmse_test = []\n",
    "\n",
    "    for train_index, test_index in k.split(data):\n",
    "\n",
    "        data_train, data_test = data.iloc[train_index, :], data.iloc[test_index, :]\n",
    "\n",
    "        model = smf.ols(formula, data=data_train).fit()\n",
    "\n",
    "        rmse_train.append(rmse(data_train[y], model.predict(data_train)))\n",
    "        rmse_test.append(rmse(data_test[y], model.predict(data_test)))\n",
    "\n",
    "    if average_rmse:\n",
    "        rmse_train = np.mean(rmse_train)\n",
    "        rmse_test = np.mean(rmse_test)\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": rmse_alldata,\n",
    "        \"R-squared\": rsquared,\n",
    "        \"BIC\": bic,\n",
    "        \"Coefficients\": n_coefficients,\n",
    "        \"Training RMSE\": rmse_train,\n",
    "        \"Test RMSE\": rmse_test,\n",
    "    }\n",
    "\n",
    "\n",
    "import statsmodels\n",
    "\n",
    "\n",
    "def point_predict_with_conf_int(\n",
    "    regression: statsmodels.regression.linear_model.RegressionResultsWrapper,\n",
    "    new_datapoint: pd.DataFrame,\n",
    "    interval_precision=0.95,\n",
    "    round_n=2,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Does point prediction and interval prediction for a new datapoint.\n",
    "\n",
    "        Parameters\n",
    "    ----------\n",
    "    regression : statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        Fitted regression model.\n",
    "    new_datapoint : pd.DataFrame\n",
    "        Database containing a new observation.\n",
    "    interval_precision : float, default=0.95\n",
    "        Precision of interval prediction.\n",
    "    round_n: int, default=2\n",
    "        Decimals to round floats in output.\n",
    "    \"\"\"\n",
    "\n",
    "    summaryframe = regression.get_prediction(new_datapoint).summary_frame(\n",
    "        alpha=1 - interval_precision\n",
    "    )\n",
    "\n",
    "    point_prediction = round(summaryframe[\"mean\"].values[0], round_n)\n",
    "\n",
    "    conf_int = [\n",
    "        round(i, round_n)\n",
    "        for i in summaryframe[[\"obs_ci_lower\", \"obs_ci_upper\"]].values[0]\n",
    "    ]\n",
    "\n",
    "    if round_n == 0:\n",
    "        point_prediction = int(point_prediction)\n",
    "        conf_int = [int(i) for i in conf_int]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        \"Point prediction\": point_prediction,\n",
    "        f\"Prediction Interval ({round(interval_precision*100)}%)\": conf_int,\n",
    "    }\n",
    "\n",
    "\n",
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class ConfintError(Error):\n",
    "    \"\"\"\n",
    "    Error raised when a confidence interval\n",
    "    does not match with required format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message=\"Confidence intervals are two numbers, so len(conf_int) must be 2.\",\n",
    "    ):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "def format_confidence_interval(conf_int: List[float], round_n=2) -> str:\n",
    "    \"\"\"Format confidence interval.\n",
    "\n",
    "        Parameters\n",
    "    ----------\n",
    "    conf_int: np.array\n",
    "        Array, consisting upper and upper confidence interval values.\n",
    "    round_n: int, default=2\n",
    "        Decimals to round floats in output.\n",
    "    \"\"\"\n",
    "    if len(conf_int) != 2:\n",
    "        raise ConfintError\n",
    "    elif round_n == 0:\n",
    "        return \"[\" + \"–\".join([str(int(round(i, round_n))) for i in conf_int]) + \"]\"\n",
    "    else:\n",
    "        return \"[\" + \"–\".join([str(round(i, round_n)) for i in conf_int]) + \"]\"\n",
    "\n",
    "\n",
    "def create_sample_frame(\n",
    "    vector: np.array, sample_size: int, n_samples=10000, with_replacement=False, seed=42\n",
    ") -> List[np.array]:\n",
    "    \"\"\"\n",
    "    Function for a specified number of samples.\n",
    "    Draws a specified number of observations from a vector, either with or without replacement.\n",
    "    Returns the matrix of samples.\n",
    "\n",
    "    Used in `ch05-stock-market-loss-generalize.ipynb`\n",
    "\n",
    "       Parameters\n",
    "    ----------\n",
    "    vector : np.array\n",
    "        Vector of observations.\n",
    "    sample_size : int\n",
    "        Sample size, you want to draw.\n",
    "        Set it len(vector) for bootstrap sampling.\n",
    "    n_samples : int, default=10000\n",
    "        Number of samples.\n",
    "    with_replacement : bool, default=False\n",
    "        Whether to perform sampling with or without\n",
    "        replacement. Set `True` for bootstrap sampling.\n",
    "    seed : int,default=42\n",
    "        Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sample_frame = np.zeros((n_samples, sample_size))\n",
    "    for i in range(n_samples):\n",
    "        sample_frame[i] = rng.choice(vector, size=sample_size, replace=with_replacement)\n",
    "\n",
    "    return sample_frame\n",
    "\n",
    "\n",
    "def add_margin(ax, x=0.05, y=0.05) -> None:\n",
    "    \"\"\"\n",
    "    This will, by default, add 5% to the x and y margins to matplotlib plots.\n",
    "    You can customise this using the x and y arguments when you call it.\n",
    "\n",
    "       Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes._subplots.AxesSubplot\n",
    "        Vector of observations.\n",
    "    x : float,default=0.05\n",
    "        Margin to add for x axis.\n",
    "    y : float,default=0.05\n",
    "        Margin to add for x axis.\n",
    "    \"\"\"\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    xmargin = (xlim[1] - xlim[0]) * x\n",
    "    ymargin = (ylim[1] - ylim[0]) * y\n",
    "\n",
    "    ax.set_xlim(xlim[0] - xmargin, xlim[1] + xmargin)\n",
    "    ax.set_ylim(ylim[0] - ymargin, ylim[1] + ymargin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f46310",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313510e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mizani.formatters import percent_format\n",
    "from plotnine import *\n",
    "from scipy.stats import logistic\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from mizani import transforms\n",
    "from stargazer.stargazer import Stargazer\n",
    "from IPython.core.display import HTML\n",
    "from stargazer.stargazer import Stargazer\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc886c30",
   "metadata": {},
   "source": [
    "## Loading datasets: hotel_features and hotel_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dad84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV files\n",
    "hotels_features = pd.read_csv('hotels-europe_features.csv')\n",
    "hotels_price= pd.read_csv('hotels-europe_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51522961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rome              4883\n",
       "Paris             2184\n",
       "London            1401\n",
       "Istanbul          1377\n",
       "Milan             1098\n",
       "Barcelona          835\n",
       "Madrid             708\n",
       "Lisbon             671\n",
       "St. Petersburg     663\n",
       "Prague             635\n",
       "Berlin             579\n",
       "Vienna             516\n",
       "Naples             509\n",
       "Krakow             431\n",
       "Amsterdam          429\n",
       "Moscow             400\n",
       "Athens             369\n",
       "Munich             360\n",
       "Warsaw             345\n",
       "Budapest           340\n",
       "Salzburg           339\n",
       "Brussels           333\n",
       "Lyon               312\n",
       "Dublin             309\n",
       "Stockholm          259\n",
       "Hamburg            257\n",
       "Seville            231\n",
       "Marseille          212\n",
       "Birmingham         185\n",
       "Dubrovnik          181\n",
       "Glasgow            162\n",
       "Kiev               134\n",
       "Bucharest          132\n",
       "Riga               131\n",
       "Copenhagen         121\n",
       "Belgrade           111\n",
       "Bratislava         104\n",
       "Helsinki           103\n",
       "Sofia              101\n",
       "Rotterdam           94\n",
       "Tallin              90\n",
       "Vilnius             88\n",
       "Zagreb              60\n",
       "Minsk               51\n",
       "Valetta             35\n",
       "Samara              34\n",
       "Name: city, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking values of hotels and countries\n",
    "hotels_features['city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a774f",
   "metadata": {},
   "source": [
    "We select **Madrid** for the analysis of this assignment since it has more than 250 values (708 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be20761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of 'Madrid' hotels in the data\n",
    "filter_test_madrid = hotels_features.loc[hotels_features[\"city\"].isin(['Madrid'])]\n",
    "filter_test_madrid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e18bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
